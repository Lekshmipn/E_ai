{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9805994,"sourceType":"datasetVersion","datasetId":6010636}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    DebertaV2Tokenizer, \n    DebertaV2ForSequenceClassification, \n    Trainer, \n    TrainingArguments, \n    get_polynomial_decay_schedule_with_warmup\n)\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom transformers import DebertaV2Tokenizer, DebertaV2ForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom scipy.special import expit  # for sigmoid in compute_metrics function\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\n\n# Define the inappropriateness dimensions and labels\nDIMS = [\n    'Inappropriateness', 'Toxic Emotions', 'Excessive Intensity', 'Emotional Deception', \n    'Missing Commitment', 'Missing Seriousness', 'Missing Openness', 'Missing Intelligibility', \n    'Unclear Meaning', 'Missing Relevance', 'Confusing Reasoning', 'Other Reasons', \n    'Detrimental Orthography', 'Reason Unclassified'\n]\n\n# Load the tokenizer and model\ntokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-large')\nmodel = DebertaV2ForSequenceClassification.from_pretrained('microsoft/deberta-v3-large', num_labels=len(DIMS))\n\n# Load and preprocess data\ntrain_df = pd.read_csv('/Dataset/train.csv')\nvalid_df = pd.read_csv('/Dataset/valid.csv')\ntest_df = pd.read_csv('/Dataset/test.csv')\n\n# Convert labels to binary format\ndef preprocess_labels(df):\n    return df[DIMS].values\n\ntrain_labels = preprocess_labels(train_df)\nvalid_labels = preprocess_labels(valid_df)\ntest_labels = preprocess_labels(test_df)\n\ntrain_texts = train_df['post_text'].tolist()\nvalid_texts = valid_df['post_text'].tolist()\ntest_texts = test_df['post_text'].tolist()\n\n# Calculate class weights to handle label imbalance\nlabel_counts = train_labels.sum(axis=0)\ntotal_counts = len(train_labels)\nclass_weights = torch.tensor(total_counts / (len(DIMS) * label_counts), dtype=torch.float)\n\n# Dataset class\nclass ArgumentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = self.texts[item]\n        label = self.labels[item]\n        \n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        \n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': torch.tensor(label, dtype=torch.float)\n        }\n\n# Define datasets\nmax_len = 256\ntrain_dataset = ArgumentDataset(train_texts, train_labels, tokenizer, max_len)\nvalid_dataset = ArgumentDataset(valid_texts, valid_labels, tokenizer, max_len)\ntest_dataset = ArgumentDataset(test_texts, test_labels, tokenizer, max_len)\n\n# Define evaluation metrics\n# Define the compute metrics function for evaluation\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom scipy.special import expit  # sigmoid function\n\n# Define the new compute_metrics function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    # Convert logits to probabilities and round to get binary predictions\n    predictions = torch.round(torch.sigmoid(torch.from_numpy(logits))).numpy()\n    \n    out_dict = {}\n    prec = 0\n    rec = 0\n    macroF1 = 0\n    \n    # Loop over each label dimension and compute scores\n    for i, dim in enumerate(DIMS):\n        scores = precision_recall_fscore_support(labels[:, i], predictions[:, i], average='macro', zero_division=0)\n        prec += scores[0]\n        rec += scores[1]\n        macroF1 += scores[2]\n        out_dict[f'{dim}_precision'] = scores[0]\n        out_dict[f'{dim}_recall'] = scores[1]\n        out_dict[f'{dim}_macroF1'] = scores[2]\n        \n    # Calculate the average precision, recall, and F1\n    out_dict['mean_precision'] = prec / len(DIMS)\n    out_dict['mean_recall'] = rec / len(DIMS)\n    out_dict['mean_F1'] = macroF1 / len(DIMS)\n    \n    return out_dict\n\n# Update training arguments with gradient accumulation, weight decay, and additional epochs\ntraining_args = TrainingArguments(\n    output_dir='./fine_tuned_results',\n    num_train_epochs=5,  # Increase epochs to improve rare label learning\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=4,  # Effective batch size of 32\n    learning_rate=2e-5,  # Lower learning rate\n    warmup_steps=500,  # Stabilize learning in initial steps\n    weight_decay=0.02,  # Regularization to prevent overfitting\n    logging_dir='./fine_tuned_logs',\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    report_to=\"none\",\n    fp16=True,  # Mixed precision for faster training\n    save_total_limit=1\n)\n\n# Initialize Trainer with class weights in the loss function\nfrom torch.nn import BCEWithLogitsLoss\n\n# Focal Loss Implementation\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2, alpha=None):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha  # Optional weighting factor for positive/negative samples\n\n    def forward(self, logits, labels):\n        bce_loss = nn.functional.binary_cross_entropy_with_logits(logits, labels, reduction='none')\n        pt = torch.exp(-bce_loss)  # Probabilities of correct predictions\n        focal_loss = (1 - pt) ** self.gamma * bce_loss\n        if self.alpha is not None:\n            alpha_t = self.alpha * labels + (1 - self.alpha) * (1 - labels)\n            focal_loss *= alpha_t\n        return focal_loss.mean()\n\n# Asymmetric Loss Implementation\nclass AsymmetricLoss(nn.Module):\n    def __init__(self, gamma_neg=4, gamma_pos=1):\n        super(AsymmetricLoss, self).__init__()\n        self.gamma_neg = gamma_neg\n        self.gamma_pos = gamma_pos\n\n    def forward(self, logits, labels):\n        probas = torch.sigmoid(logits)\n        pos_loss = (1 - probas) ** self.gamma_pos * labels * torch.log(probas + 1e-8)\n        neg_loss = probas ** self.gamma_neg * (1 - labels) * torch.log(1 - probas + 1e-8)\n        loss = -torch.mean(pos_loss + neg_loss)\n        return loss\n\n# Weighted Trainer Class\nclass WeightedTrainer(Trainer):\n    def __init__(self, *args, loss_fn=\"focal\", **kwargs):\n        super().__init__(*args, **kwargs)\n        self.loss_fn = loss_fn\n        if loss_fn == \"focal\":\n            self.criterion = FocalLoss(gamma=2)  # Adjust gamma as needed\n        elif loss_fn == \"asl\":\n            self.criterion = AsymmetricLoss(gamma_neg=4, gamma_pos=1)  # Adjust gammas as needed\n        else:\n            # Default to BCEWithLogitsLoss with class weights\n            self.criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights.to(self.model.device))\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss = self.criterion(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n# Initialize weighted Trainer\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    loss_fn=\"asl\"  \n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the model on the validation set\nresults = trainer.evaluate(valid_dataset)\nprint(f\"Validation results: {results}\")\n\n# Evaluate on the test set for final performance\ntest_results = trainer.evaluate(test_dataset)\nprint(f\"Test results: {test_results}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T19:44:55.030713Z","iopub.execute_input":"2024-11-16T19:44:55.031117Z","iopub.status.idle":"2024-11-16T20:01:36.101250Z","shell.execute_reply.started":"2024-11-16T19:44:55.031070Z","shell.execute_reply":"2024-11-16T20:01:36.100372Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [240/240 16:09, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Inappropriateness Precision</th>\n      <th>Inappropriateness Recall</th>\n      <th>Inappropriateness Macrof1</th>\n      <th>Toxic emotions Precision</th>\n      <th>Toxic emotions Recall</th>\n      <th>Toxic emotions Macrof1</th>\n      <th>Excessive intensity Precision</th>\n      <th>Excessive intensity Recall</th>\n      <th>Excessive intensity Macrof1</th>\n      <th>Emotional deception Precision</th>\n      <th>Emotional deception Recall</th>\n      <th>Emotional deception Macrof1</th>\n      <th>Missing commitment Precision</th>\n      <th>Missing commitment Recall</th>\n      <th>Missing commitment Macrof1</th>\n      <th>Missing seriousness Precision</th>\n      <th>Missing seriousness Recall</th>\n      <th>Missing seriousness Macrof1</th>\n      <th>Missing openness Precision</th>\n      <th>Missing openness Recall</th>\n      <th>Missing openness Macrof1</th>\n      <th>Missing intelligibility Precision</th>\n      <th>Missing intelligibility Recall</th>\n      <th>Missing intelligibility Macrof1</th>\n      <th>Unclear meaning Precision</th>\n      <th>Unclear meaning Recall</th>\n      <th>Unclear meaning Macrof1</th>\n      <th>Missing relevance Precision</th>\n      <th>Missing relevance Recall</th>\n      <th>Missing relevance Macrof1</th>\n      <th>Confusing reasoning Precision</th>\n      <th>Confusing reasoning Recall</th>\n      <th>Confusing reasoning Macrof1</th>\n      <th>Other reasons Precision</th>\n      <th>Other reasons Recall</th>\n      <th>Other reasons Macrof1</th>\n      <th>Detrimental orthography Precision</th>\n      <th>Detrimental orthography Recall</th>\n      <th>Detrimental orthography Macrof1</th>\n      <th>Reason unclassified Precision</th>\n      <th>Reason unclassified Recall</th>\n      <th>Reason unclassified Macrof1</th>\n      <th>Mean Precision</th>\n      <th>Mean Recall</th>\n      <th>Mean F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.109000</td>\n      <td>0.110965</td>\n      <td>0.218182</td>\n      <td>0.500000</td>\n      <td>0.303797</td>\n      <td>0.361364</td>\n      <td>0.500000</td>\n      <td>0.419525</td>\n      <td>0.090909</td>\n      <td>0.500000</td>\n      <td>0.153846</td>\n      <td>0.452460</td>\n      <td>0.463068</td>\n      <td>0.261439</td>\n      <td>0.334091</td>\n      <td>0.500000</td>\n      <td>0.400545</td>\n      <td>0.459091</td>\n      <td>0.500000</td>\n      <td>0.478673</td>\n      <td>0.350000</td>\n      <td>0.500000</td>\n      <td>0.411765</td>\n      <td>0.550194</td>\n      <td>0.547941</td>\n      <td>0.548421</td>\n      <td>0.395455</td>\n      <td>0.500000</td>\n      <td>0.441624</td>\n      <td>0.885845</td>\n      <td>0.509804</td>\n      <td>0.454798</td>\n      <td>0.480641</td>\n      <td>0.432483</td>\n      <td>0.353491</td>\n      <td>0.596061</td>\n      <td>0.612440</td>\n      <td>0.603365</td>\n      <td>0.521277</td>\n      <td>0.575472</td>\n      <td>0.171964</td>\n      <td>0.493182</td>\n      <td>0.500000</td>\n      <td>0.496568</td>\n      <td>0.442054</td>\n      <td>0.510086</td>\n      <td>0.392844</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.091100</td>\n      <td>0.090886</td>\n      <td>0.279817</td>\n      <td>0.491935</td>\n      <td>0.356725</td>\n      <td>0.658095</td>\n      <td>0.684452</td>\n      <td>0.663300</td>\n      <td>0.397751</td>\n      <td>0.368056</td>\n      <td>0.263786</td>\n      <td>0.476165</td>\n      <td>0.465909</td>\n      <td>0.376005</td>\n      <td>0.677273</td>\n      <td>0.699888</td>\n      <td>0.667879</td>\n      <td>0.867442</td>\n      <td>0.608636</td>\n      <td>0.655927</td>\n      <td>0.637500</td>\n      <td>0.651515</td>\n      <td>0.641226</td>\n      <td>0.600818</td>\n      <td>0.548935</td>\n      <td>0.405664</td>\n      <td>0.664678</td>\n      <td>0.667291</td>\n      <td>0.665954</td>\n      <td>0.587655</td>\n      <td>0.605117</td>\n      <td>0.591089</td>\n      <td>0.461187</td>\n      <td>0.497537</td>\n      <td>0.478673</td>\n      <td>0.475000</td>\n      <td>0.500000</td>\n      <td>0.487179</td>\n      <td>0.481818</td>\n      <td>0.500000</td>\n      <td>0.490741</td>\n      <td>0.493182</td>\n      <td>0.500000</td>\n      <td>0.496568</td>\n      <td>0.554170</td>\n      <td>0.556376</td>\n      <td>0.517194</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.085700</td>\n      <td>0.085223</td>\n      <td>0.447773</td>\n      <td>0.497144</td>\n      <td>0.367872</td>\n      <td>0.709761</td>\n      <td>0.725333</td>\n      <td>0.716129</td>\n      <td>0.696078</td>\n      <td>0.668056</td>\n      <td>0.679743</td>\n      <td>0.714821</td>\n      <td>0.707386</td>\n      <td>0.710945</td>\n      <td>0.708120</td>\n      <td>0.726913</td>\n      <td>0.712520</td>\n      <td>0.974178</td>\n      <td>0.694444</td>\n      <td>0.766747</td>\n      <td>0.664861</td>\n      <td>0.679654</td>\n      <td>0.669670</td>\n      <td>0.642857</td>\n      <td>0.656013</td>\n      <td>0.636779</td>\n      <td>0.606393</td>\n      <td>0.614568</td>\n      <td>0.609870</td>\n      <td>0.615610</td>\n      <td>0.632672</td>\n      <td>0.621139</td>\n      <td>0.842593</td>\n      <td>0.585772</td>\n      <td>0.624957</td>\n      <td>0.475000</td>\n      <td>0.500000</td>\n      <td>0.487179</td>\n      <td>0.481818</td>\n      <td>0.500000</td>\n      <td>0.490741</td>\n      <td>0.493182</td>\n      <td>0.500000</td>\n      <td>0.496568</td>\n      <td>0.648075</td>\n      <td>0.620568</td>\n      <td>0.613633</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.080900</td>\n      <td>0.082181</td>\n      <td>0.659722</td>\n      <td>0.511593</td>\n      <td>0.391765</td>\n      <td>0.735681</td>\n      <td>0.751160</td>\n      <td>0.742267</td>\n      <td>0.713482</td>\n      <td>0.755556</td>\n      <td>0.729840</td>\n      <td>0.671245</td>\n      <td>0.693182</td>\n      <td>0.680233</td>\n      <td>0.692454</td>\n      <td>0.716988</td>\n      <td>0.682405</td>\n      <td>0.806288</td>\n      <td>0.790704</td>\n      <td>0.798236</td>\n      <td>0.671419</td>\n      <td>0.703463</td>\n      <td>0.664488</td>\n      <td>0.626074</td>\n      <td>0.612586</td>\n      <td>0.538765</td>\n      <td>0.620000</td>\n      <td>0.679910</td>\n      <td>0.600224</td>\n      <td>0.580833</td>\n      <td>0.612542</td>\n      <td>0.561401</td>\n      <td>0.567674</td>\n      <td>0.614894</td>\n      <td>0.579082</td>\n      <td>0.977169</td>\n      <td>0.545455</td>\n      <td>0.571651</td>\n      <td>0.984018</td>\n      <td>0.562500</td>\n      <td>0.602990</td>\n      <td>0.493182</td>\n      <td>0.500000</td>\n      <td>0.496568</td>\n      <td>0.699946</td>\n      <td>0.646467</td>\n      <td>0.617137</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.072300</td>\n      <td>0.081570</td>\n      <td>0.725287</td>\n      <td>0.705813</td>\n      <td>0.708255</td>\n      <td>0.750490</td>\n      <td>0.750490</td>\n      <td>0.750490</td>\n      <td>0.711242</td>\n      <td>0.745833</td>\n      <td>0.725343</td>\n      <td>0.686418</td>\n      <td>0.701705</td>\n      <td>0.693223</td>\n      <td>0.723232</td>\n      <td>0.723232</td>\n      <td>0.723232</td>\n      <td>0.849593</td>\n      <td>0.795655</td>\n      <td>0.819820</td>\n      <td>0.697949</td>\n      <td>0.701299</td>\n      <td>0.699542</td>\n      <td>0.638659</td>\n      <td>0.649964</td>\n      <td>0.619258</td>\n      <td>0.665057</td>\n      <td>0.724263</td>\n      <td>0.675121</td>\n      <td>0.612500</td>\n      <td>0.646189</td>\n      <td>0.614121</td>\n      <td>0.617502</td>\n      <td>0.617502</td>\n      <td>0.617502</td>\n      <td>0.781395</td>\n      <td>0.631579</td>\n      <td>0.675708</td>\n      <td>0.650538</td>\n      <td>0.557783</td>\n      <td>0.580420</td>\n      <td>0.493182</td>\n      <td>0.500000</td>\n      <td>0.496568</td>\n      <td>0.685932</td>\n      <td>0.675093</td>\n      <td>0.671329</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='83' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [28/28 00:24]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Validation results: {'eval_loss': 0.08157048374414444, 'eval_Inappropriateness_precision': 0.725287356321839, 'eval_Inappropriateness_recall': 0.7058131720430108, 'eval_Inappropriateness_macroF1': 0.708254527272332, 'eval_Toxic Emotions_precision': 0.7504897412104341, 'eval_Toxic Emotions_recall': 0.7504897412104341, 'eval_Toxic Emotions_macroF1': 0.7504897412104341, 'eval_Excessive Intensity_precision': 0.7112423916935195, 'eval_Excessive Intensity_recall': 0.7458333333333333, 'eval_Excessive Intensity_macroF1': 0.7253433208489388, 'eval_Emotional Deception_precision': 0.6864184270199307, 'eval_Emotional Deception_recall': 0.7017045454545454, 'eval_Emotional Deception_macroF1': 0.6932230175699545, 'eval_Missing Commitment_precision': 0.7232317584568073, 'eval_Missing Commitment_recall': 0.7232317584568073, 'eval_Missing Commitment_macroF1': 0.7232317584568073, 'eval_Missing Seriousness_precision': 0.8495934959349594, 'eval_Missing Seriousness_recall': 0.7956545654565457, 'eval_Missing Seriousness_macroF1': 0.8198198198198198, 'eval_Missing Openness_precision': 0.6979489164086687, 'eval_Missing Openness_recall': 0.7012987012987013, 'eval_Missing Openness_macroF1': 0.6995415081455467, 'eval_Missing Intelligibility_precision': 0.6386593204775023, 'eval_Missing Intelligibility_recall': 0.6499638858793788, 'eval_Missing Intelligibility_macroF1': 0.6192581163076796, 'eval_Unclear Meaning_precision': 0.6650574712643679, 'eval_Unclear Meaning_recall': 0.7242628685657171, 'eval_Unclear Meaning_macroF1': 0.6751211171273866, 'eval_Missing Relevance_precision': 0.6125, 'eval_Missing Relevance_recall': 0.6461886529759833, 'eval_Missing Relevance_macroF1': 0.6141209021962005, 'eval_Confusing Reasoning_precision': 0.6175021732831063, 'eval_Confusing Reasoning_recall': 0.6175021732831063, 'eval_Confusing Reasoning_macroF1': 0.6175021732831063, 'eval_Other Reasons_precision': 0.7813953488372093, 'eval_Other Reasons_recall': 0.631578947368421, 'eval_Other Reasons_macroF1': 0.6757075471698113, 'eval_Detrimental Orthography_precision': 0.6505376344086021, 'eval_Detrimental Orthography_recall': 0.5577830188679245, 'eval_Detrimental Orthography_macroF1': 0.5804195804195804, 'eval_Reason Unclassified_precision': 0.49318181818181817, 'eval_Reason Unclassified_recall': 0.5, 'eval_Reason Unclassified_macroF1': 0.4965675057208238, 'eval_mean_precision': 0.6859318466784831, 'eval_mean_recall': 0.6750932402995647, 'eval_mean_F1': 0.6713286168248872, 'eval_runtime': 8.326, 'eval_samples_per_second': 26.423, 'eval_steps_per_second': 3.363, 'epoch': 5.0}\nTest results: {'eval_loss': 0.07615890353918076, 'eval_Inappropriateness_precision': 0.758670926517572, 'eval_Inappropriateness_recall': 0.7111737089201878, 'eval_Inappropriateness_macroF1': 0.7013264116495457, 'eval_Toxic Emotions_precision': 0.7447670973094702, 'eval_Toxic Emotions_recall': 0.7720028450251574, 'eval_Toxic Emotions_macroF1': 0.75495437252903, 'eval_Excessive Intensity_precision': 0.6715512673935473, 'eval_Excessive Intensity_recall': 0.714804469273743, 'eval_Excessive Intensity_macroF1': 0.6862826644486033, 'eval_Emotional Deception_precision': 0.6797139830508474, 'eval_Emotional Deception_recall': 0.7416666666666667, 'eval_Emotional Deception_macroF1': 0.69765906362545, 'eval_Missing Commitment_precision': 0.7450365726227794, 'eval_Missing Commitment_recall': 0.7649405672590326, 'eval_Missing Commitment_macroF1': 0.7510285170781672, 'eval_Missing Seriousness_precision': 0.7016430171769978, 'eval_Missing Seriousness_recall': 0.6492537313432836, 'eval_Missing Seriousness_macroF1': 0.6702068637552508, 'eval_Missing Openness_precision': 0.7344161545215101, 'eval_Missing Openness_recall': 0.7644087938205586, 'eval_Missing Openness_macroF1': 0.7422756535985418, 'eval_Missing Intelligibility_precision': 0.7054922556524835, 'eval_Missing Intelligibility_recall': 0.71375, 'eval_Missing Intelligibility_macroF1': 0.6526131240607782, 'eval_Unclear Meaning_precision': 0.6558513465627215, 'eval_Unclear Meaning_recall': 0.7210668509675797, 'eval_Unclear Meaning_macroF1': 0.6593662978851509, 'eval_Missing Relevance_precision': 0.6587009934836022, 'eval_Missing Relevance_recall': 0.7167366946778712, 'eval_Missing Relevance_macroF1': 0.6553894571203778, 'eval_Confusing Reasoning_precision': 0.5501013700313325, 'eval_Confusing Reasoning_recall': 0.5578163771712159, 'eval_Confusing Reasoning_macroF1': 0.5533552631578947, 'eval_Other Reasons_precision': 0.7131221719457014, 'eval_Other Reasons_recall': 0.6344638574854402, 'eval_Other Reasons_macroF1': 0.663406455218667, 'eval_Detrimental Orthography_precision': 0.78729792147806, 'eval_Detrimental Orthography_recall': 0.6047843665768193, 'eval_Detrimental Orthography_macroF1': 0.6503101394091997, 'eval_Reason Unclassified_precision': 0.4920091324200913, 'eval_Reason Unclassified_recall': 0.5, 'eval_Reason Unclassified_macroF1': 0.49597238204833144, 'eval_mean_precision': 0.6855981578690514, 'eval_mean_recall': 0.683347780656254, 'eval_mean_F1': 0.6667247618274991, 'eval_runtime': 16.5354, 'eval_samples_per_second': 26.489, 'eval_steps_per_second': 3.326, 'epoch': 5.0}\n","output_type":"stream"}],"execution_count":3}]}